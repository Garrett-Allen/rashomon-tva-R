---
title: "rashomon_package"
author: "garrett-allen"
format: html
editor: visual
---

```{r}
library(rashomontva)
library(tidyverse)
library(collections)
library(data.table)
```

#todo add examples

```{r}

#M = 2
#R = c(3,4)

data <- data.frame(arm1 = c(1,1,1,1,1,1,1,1,1,2,2,3,3,3,2,2,2,2,2,2,3,3,3,3,3,3,3), 
                    arm2 = c(1,2,3,1,2,3,1,2,3,1,2,3,1,2,4,1,2,3,1,2,3,4,2,4,1,2,4), 
                    value = runif(27))
sigma <- initialize_sigma(2, c(5,5))

data <- assign_policy_label(data,arm1,arm2)

policy_list <- create_policies_from_data(data, arm1, arm2)

edge_list <- lattice_edges(sigma, policy_list)

M <- policy_means(data, value)

# use as.integer


```

```{r}
#| label: main loop


#assumptions about data here: 
#data has been assigned policy labels 
#we have list of policies from data
#we will filter data for the that fit the profile
#after filtering for policies in profile, pull out labels from policy_list
#this is to use for filtering data

#number in policy_list will correspond to policy_label in data. 


find_rashomon_profile <- function(data, value, M,R,H, reg=1,
                                  profile,
                                  policies = c(),
                                  policy_means=c(), 
                                  normalize=0,
                                  theta = Inf){
  
  if(max(R) == 2){
    sigma = matrix(nrow = M, ncol = 1)
    rashomon_set = RashomonSet(models = list(sigma), 
                               losses = numeric(),
                               pools = list(1),
                               profiles = list(list(profile)))
    return(rashomon_set)
  }
  
  sigma = initialize_sigma(M, R)
  hasse_edges = lattice_edges(sigma, policies)
  #creating list for compatibility
  if(length(R) == 1){
    R = rep(R, M)
  }
  
  #defining list, hash tables, and queue
  rashomon_set = RashomonSet(models = list(), 
                             losses = numeric(),
                             pools = list(), 
                             profiles = list())
  seen_sigmas = hashtab()
  seen_sigmas_sub = hashtab()
  queue_sigma = collections::queue(items = NULL)
  #pushing initial problems onto queue
  for(i in 1:M){
    if(!is.na(sigma[i,1])){
      queue_sigma$push(list(sigma,i,0))
    }
  }
  count = 0
  #main queue loop

  while(queue_sigma$size() > 0){
    count = count + 1
    sigma_list = queue_sigma$pop()
    
    sigma = sigma_list[[1]]
    i = sigma_list[[2]]
    j = sigma_list[[3]]
    sethash(seen_sigmas_sub, sigma_list, 1)
    
    if(num_pools(sigma,R) > H)
      next
    
    sigma_1 = sigma
    sigma_0 = sigma
    sigma_1[i,j] = 1
    sigma_0[i,j] = 0
    
    #adding all subproblem variants
    for(m in 1:M){
      
      R_m = R[m]
      j1 = 1
      sigma_1_list = list(sigma_1, m, j1)
      
      #have we seen this subproblem?
      seen = gethash(seen_sigmas_sub, sigma_1_list, nomatch = 0)
     
      
      #if we have seen it, and our cutting index is less than R_m - 2, consider further cuts
      while(seen != 0 & j1 <= R_m - 2){
        j1 = j1 + 1
        sigma_1_list = list(sigma_1,m,j1)
        seen = gethash(seen_sigmas_sub, sigma_1_list, nomatch = 0)
      }
      
      #if we haven't seen it and cutting is at a vlaid range, add it as a subproblem
      if(j1 <= R_m -2 & seen == 0){
        queue_sigma$push(sigma_1_list)
        sethash(seen_sigmas_sub, sigma_1_list,1)
      }
      
      j0 = 1
      sigma_0_list = list(sigma_0, m, j0)
      seen = gethash(seen_sigmas_sub, sigma_0_list, nomatch = 0)
      
      while(seen != 0 & j0 <= R_m - 2){
        j0 = j0 + 1
        sigma_0_list = list(sigma_0, m, j0)
        seen = gethash(seen_sigmas_sub, sigma_0_list, nomatch = 0)
      }
      
      if(j0 <= R_m -2 & seen == 0){
        queue_sigma$push(sigma_0_list)
        sethash(seen_sigmas_sub, sigma_0_list,1)
      }
     
    }
    
    #computing B to check whether more splits is possible
    
    #i had to add this condition... why does apara's code not need it?
    if(j != ncol(sigma)){
      B = compute_B(data, {{value}}, i,j, policy_means, sigma, policies, reg = 1, normalize = 0,
                  lattice_edges = hasse_edges,R)
    
      if(B > theta){
        next
    }
      
    }
    
    #Check if unsplit pool satisfies Rashomon threshold
    if(gethash(seen_sigmas, sigma_1, nomatch = 0) == 0){
      sethash(seen_sigmas, sigma_1,1)
      
      Q = compute_loss(data, {{value}}, policy_means, sigma_1, policies, reg = 1, normalize = 0,
                       lattice_edges = hasse_edges,R)

      if(Q <= theta){
        
        rashomon_set$insert_sigma(sigma_1, Q)
        num_pools = num_pools(sigma_1, R)
        rashomon_set$insert_pools(num_pools)
        rashomon_set$insert_profile(list(profile))
      }
    }
    #Check if split pool satisfies Rashomon threshold
    if(gethash(seen_sigmas, sigma_0, nomatch = 0) == 0 & num_pools(sigma_0,R) <= H){
      sethash(seen_sigmas, sigma_0,1)
  
      Q = compute_loss(data, {{value}}, policy_means, sigma_0, policies, reg = 1, normalize = 0,
                       lattice_edges = hasse_edges,R)
      if(Q <= theta){
        
        rashomon_set$insert_sigma(sigma_0, Q)
        num_pools = num_pools(sigma_0, R)
        rashomon_set$insert_pools(num_pools)
        rashomon_set$insert_profile(list(profile))
      }
    }
    
    sigma_1_list = list(sigma_1, i, j+1)
    sigma_0_list = list(sigma_0, i, j+1)
    
    #add child problems
    if(j+1 < R[i] - 2){
      if(gethash(seen_sigmas_sub, sigma_1_list, nomatch = 0) == 0){
        queue_sigma$push(list(sigma_1,i, j + 1))
      }
      if(gethash(seen_sigmas_sub, sigma_0_list, nomatch = 0) == 0){
        queue_sigma$push(list(sigma_0,i, j + 1))
      }
    }
    
  }
  rashomon_set
  
  
}

# res <- find_rashomon_profile(data = data, 
#                      value = value, 
#                      M = 2, 
#                      R = c(3,4),
#                      H = Inf, 
#                      profile = c(1,1),
#                      policies = policy_list,
#                      policy_means = M)


```

```{r}
#| label: test

#TODO: 

# Write functin to compute rashomon set for a particular profile
# Write main loop to do this for all profiles
# Write function to combine all profiles to get the actual rashomon set

#return rashomon set object from which you can then pass this object into 
#predict, other functions

#make this package CRAN ready
# https://bookdown.dongzhuoer.com/hadley/r-pkgs/release
```

  # Add problem variants to queue
        for m in range(M):
            R_m = R[m]
            j1 = 0
            while problems.seen(sigma_1, m, j1) and j1 < R_m - 3:
                j1 += 1
            if j1 <= R_m - 3:
                queue.append((sigma_1, m, j1))
            j0 = 0
            while problems.seen(sigma_0, m, j0) and j0 < R_m - 3:
                j0 += 1
            if j0 <= R_m - 3:
                queue.append((sigma_0, m, j0))



```{r}


#calculates which profiles are active
pol_in_prof <- function(policies, profile, inactive = 0){
  
  policy_profiles = lapply(policies, function(x) x != inactive)
  sapply(policy_profiles, function(x) all(x == profile))
  
}

#subset data
subset_prof <- function(data, policy_list, profile,inactive = 0){
  
  policies_in_profile = pol_in_prof(policy_list, profile, inactive)
  subset(data, policies_in_profile[data$policy_label])
   
  
}

#data
#M number of features
#R vector of factor levels
#H Maximum number of pools
#theta Rashomon threshold


insert_sigma <- function(new_sigma, loss){
  
  models <<- append(models, list(new_sigma))
  losses <<- c(losses,loss)
}

insert_pools <- function(pool_num){
  pools <<- append(pools, pool_num)
}

insert_profile <- function(profile){
  profiles <<- append(profiles, profile)
}

sort <- function(){
  order <- order(losses)
  losses <<- losses[order]
  models <<- models[order]
  pools  <<- pools[order]
  profiles <<- profiles[order]
}


RashomonSet <- setRefClass("RashomonSet", fields = list(models = "list",
                                                        losses = "numeric",
                                                        pools = "list",
                                                        profiles = "list"),
                           methods = list(insert_sigma = insert_sigma,
                                          insert_pools = insert_pools,
                                          insert_profile = insert_profile,
                                          sort = sort))

```


```{r}



find_profile_lower_bound <- function(data, value){
  
  n_k = nrow(data)
  data_mean <- data %>% 
    group_by(policy_label) %>% 
    mutate(mean = mean({{value}}))
  
  (yardstick::rmse_vec(pull(data_mean,{{value}}), data_mean$mean))^2 * n_k
  
}

aggregate_rashomon_profiles <- function(data,
                                        ...,
                                        M, 
                                        H, 
                                        R, 
                                        value, 
                                        theta, 
                                        reg = 1, 
                                        bruteforce = FALSE
                                        ){
  
  num_profiles = 2^M
  profiles = expand.grid(replicate(M, 0:1, simplify = FALSE))
  #creating policy labels on data as well as list of all policies
  
  if(length(R) == 1){
    R = rep(R,M)
  }
  
  data_labeled <- assign_policy_label(data,...)
  policy_list <- create_policies_from_data(data_labeled,...)
  num_data <- nrow(data_labeled)
  data_labeled$id <- 1:num_data
  
  #Maximum number of pools for a profile derived from maximum number of pools
  H_profile = H - num_profiles + 1
  data_profile_ids = rep(0, num_data)
  
  #initialize storage objects
  D_profile = list()
  eq_lb_profiles = rep(0, num_profiles)
  rashomon_profiles = rep(0, num_profiles)
  loss_object = rep(0, num_profiles)
  
  #Assign profile ids to each data point
  for(i in 1:num_profiles){
    
    #assign profile labels and extract appropriate subset
    profile_i = as.numeric(profiles[i,])
    data_i = subset_prof(data_labeled, policy_list, profile_i, 1)
    
    #store profile for each observation and subset corresponding to that profile
    data_profile_ids[data_i$id] = i
    D_profile[i] = list(data_i$id)
    
    #if no policies correspond to that profile
    if(length(data_i) == 0){
      eq_lb_profiles[i] = 0
      H_profile = H_profile + 1
    }
    
    else{
      eq_lb_profiles[i] = find_profile_lower_bound(data_i, {{value}})
    }
    
  }
  
  eq_lb_profiles =  eq_lb_profiles / num_data
  eq_lb_sum = sum(eq_lb_profiles)
  
  #deal with control separately 
  control_loss = eq_lb_profiles[[1]] + reg
  rashomon_profiles[1] = list(RashomonSet(models = list(NA),
                                     losses = control_loss,
                                     pools = list(1),
                                     profiles = list(as.numeric(profiles[1,]))))
  
  for(i in 2:num_profiles){
    
    #extracting relevant things to find rashomon set for this profile
    data_i = data_labeled[unlist(D_profile[i]),]
    
    if(nrow(data_i) == 0){
      rashomon_profiles[i] = list(RashomonSet(models = list(NA),
                                     losses = 0,
                                     pools = list(0),
                                     profiles = list(as.numeric(profiles[1,]))))
      next
    }
    
    profile_i = as.numeric(profiles[i,])
    M_i = sum(profile_i)
    R_i = R[as.logical(profile_i)]
    
    #find profile_lower_bound to find theta_k
    #lower_bound = compute_mse_loss(data,{{value}},)
    theta_k = theta - (eq_lb_sum - eq_lb_profiles[i])

    data_i = assign_policy_label(data_i, ...)
    policy_list_i = create_policies_from_data(data_i, ...)
    policy_list_i_masked = lapply(policy_list_i, function(x) x[as.logical(profile_i)])
    means_i = policy_means(data_i, {{value}})

    rashomon_i = find_rashomon_profile(data_i, 
                          {{value}}, 
                          M_i, 
                          R_i, 
                          H_profile, 
                          reg, 
                          profile_i,
                          policies = policy_list_i_masked, 
                          policy_means = means_i, 
                          normalize = 0, 
                          theta = theta_k)
    
    invisible(rashomon_i$sort())
    rashomon_profiles[i] = list(rashomon_i)
    
    
  }
  
  R_set = find_feasible_combinations(rashomon_profiles, theta, H, sorted = TRUE)
  
  list(R_set, rashomon_profiles)
  
}



find_feasible_combinations <- function(rashomon_profiles, theta, H, sorted = FALSE){
  
  #sorting so we can pass into the feasible_sums function
  if(!sorted){
    for(rset in rashomon_profiles){
      rset$sort()
    }
  }
  
  #coalescing all losses into one list of lists.
  all_losses <- lapply(rashomon_profiles, function(x) x$losses)
  
  loss_combinations = find_feasible_sum_subsets(all_losses, theta)
  
  feasible_combinations = list()
  
  #filtering so that we only have combinations with number of pools smaller
  #than H
  
  for(i in 1:length(loss_combinations)){
    pools = 0
    comb = loss_combinations[[i]]
    for(j in 1:length(comb)){
    
      r_prof = rashomon_profiles[[j]]
      model_id = comb[[j]]
      
      if(all(is.na(r_prof$models[[model_id]]))){
        if(r_prof$losses[[model_id]] > 0){
          pools = pools + 1
        }
      }
      
      else{
        pools = pools + r_prof$pools[[model_id]]
      }
      
      
    }
    
    if(pools <= H){
      feasible_combinations = append(feasible_combinations, list(comb))
    }
    
  }
  
  feasible_combinations
}


find_feasible_sum_subsets <- function(S,theta){
  
  numsets = length(S)
  
  if(numsets == 0){
    return(list())
  }
  
  S1 = S[[1]]
  S1_feasible_ids = which(S1 <= theta)
  
  if(numsets == 1){
    feasible_combs = lapply(S1_feasible_ids, list)
    return(feasible_combs)
  }
  
  # Since the list is sorted, add the first elements of the remaining sets
  # Then use this to check feasibility of indices in S1_feasible_idx
  first_element_sum = 0
  
  for(x in S[2:numsets]){
    first_element_sum = first_element_sum + x[1]
  }
  
  feasible_combs = list()
  for(i in S1_feasible_ids){
    theta_i = theta - S1[i]
    
    if(first_element_sum > theta_i){
      next
    }
    
    subproblem_res_i = find_feasible_sum_subsets(S[2:numsets], theta_i)
    subproblem_res_i = lapply(subproblem_res_i, function(x) c(i,x))

    feasible_combs = append(feasible_combs, subproblem_res_i)
  }
  
  feasible_combs
  
}


rset <- aggregate_rashomon_profiles(data, 
                            arm1,
                            arm2,
                            M = 2, 
                            H = Inf, 
                            R = c(4,4), 
                            value = value, 
                            theta = Inf)

# TODO: Make predictions from this output
# TODO: Test
# TODO: Deal with ids in a smart way

but otherwise, mostly done

```

```{r}
data.frame(spp1 = c("plant_1","plant_3"),
           cc1 = c("p","r"),
           spp2 = c("plant_2","plant_4"),
           cc2 = c("p","r")) %>% 
  mutate(id = 1:n())  
```

